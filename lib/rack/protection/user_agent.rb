require 'rack/protection'

module Rack
  module Protection
    ##
    # Prevented attack::   Script kiddies.
    # Supported browsers:: all
    # More infos::         http://www.askapache.com/htaccess/blocking-bad-bots-and-scrapers-with-htaccess.html
    #
    # Do not allow access for UAs known to be used by scripts.
    #
    class UserAgent < Base
      DISALLOWED_USER_AGENTS = ["webbandit", "2icommerce", "accoona", "activetouristbot", "adressendeutschland", "aipbot", "alexibot", "alligator", "allsubmitter", "almaden", "anarchie", "anonymous", "apexoo", "aqua_products", "asterias", "assort", "athens", "athome", "atomz", "attache", "autoemailspider", "autohttp", "b2w", "bew", "backdoorbot", "badass", "baiduspider", "baiduspider+", "becomebot", "berts", "bitacle", "biz360", "black.hole", "blackwidow", "bladder fusion", "blog checker", "blogpeople", "blogshares spiders", "bloodhound", "blowfish", "board bot", "bookmark search tool", "botalot", "botrighthere", "bot mailto:craftbot@yahoo.com", "bropwers", "browsezilla", "builtbottough", "bullseye", "bunnyslippers", "cegbfeieh", "cfnetwork", "cheesebot", "cherrypicker", "crescent", "charlotte/", "chinaclaw", "convera", "copernic", "copyrightcheck", "cosmos", "crescent", "c-spider", "curl", "custo", "cyberz", "datacha0s", "daum", "deweb", "digger", "digimarc", "digout4uagent", "diibot", "disco", "dittospyder", "dnloadmage", "download", "dragonfly", "dreampassport", "dsurf", "dts agent", "dumbot", "dynaweb", "e-collector", "easydl", "ebrowse", "ecatch", "ecollector", "edgeio", "efp@gmx.net", "eirgrabber", "email extractor", "emailcollector", "emailsiphon", "emailwolf", "emeraldshield", "enterprise_search", "erocrawler", "esurf", "eval", "everest-vulcan", "exabot", "express", "extractor", "extractorpro", "eyenetie", "fairad", "fastlwspider", "fetch", "fezhead", "filehound", "findlinks", "flaming attackbot", "flashget", "flickbot", "foobot", "forex", "franklin locator", "freshdownload", "frontpage", "fsurf", "gaisbot", "gamespy_arcade", "geniebot", "getbot", "getleft", "getright", "getweb!", "go!zilla", "go-ahead-got-it", "goforitbot", "grabnet", "grafula", "grub", "harvest", "hatena antenna", "heritrix", "hloader", "hmview", "holmes", "hoowwwer", "houxoucrawler", "httpget", "httplib", "httpretriever", "httrack", "humanlinks", "ibm_planetwide", "iccrawler", "ichiro", "igetter", "image stripper", "image sucker", "imagefetch", "imds_monitor", "incywincy", "industry program", "indy", "ineturl", "infonavirobot", "installshield digitalwizard", "interget", "irlbot", "iron33", "isspider", "iupui research bot", "jakarta", "java/", "jbh agent", "jennybot", "jetcar", "jeteye", "jeteyebot", "jobo", "joc web spider", "kapere", "kenjin", "keyword density", "kretrieve", "ksoap", "kwebget", "lapozzbot", "larbin", "leech", "leechftp", "leechget", "leipzig.de", "lexibot", "libweb", "libwww-fm", "libwww-perl", "lightningdownload", "linkextractorpro", "linkie", "linkscan", "linktiger", "linkwalker", "lmcrawler", "lnspiderguy", "localcombot", "looksmart", "lwp", "mac finder", "mail sweeper", "mark.blonin", "masagool", "mass", "mata hari", "mcspider", "metaproducts download express", "microsoft data access", "microsoft url control", "midown", "miixpc", "mirror", "missauga", "missouri college browse", "mister", "monster", "mkdb", "moget", "moreoverbot", "mothra/netscan", "movabletype", "mozi!", "mozilla/22", "mozilla/3.0 (compatible)", "mozilla/5.0 (compatible; msie 5.0)", "msie_6.0", "msiecrawler", "msproxy", "mvaclient", "myfamilybot", "mygetright", "nameprotect", "nasa search", "naver", "navroad", "nearsite", "netants", "netattache", "netcarta", "netmechanic", "netresearchserver", "netspider", "netzip", "net vampire", "newt activex", "nextopia", "nicerspro", "ninja", "nimblecrawler", "noxtrumbot", "npbot", "octopus", "offline", "ok mozilla", "omniexplorer", "opal", "openbot", "openfind", "opentextsitecrawler", "oracle ultra search", "outfoxbot", "p3p", "packrat", "pagegrabber", "pagmiedownload", "panscient", "papa foto", "pavuk", "pcbrowser", "perl", "perman", "personapilot", "php version", "plantynet_webrobot", "playstarmusic", "plucker", "port huron", "program shareware", "progressive download", "propowerbot", "prospector", "prowebwalker", "prozilla", "psbot", "psycheclone", "puf", "pushsite", "pussycat", "puxarapido", "python-urllib", "quepasacreep", "queryn", "radiation", "realdownload", "redcarpet", "redkernel", "reget", "relevantnoise", "repomonkey", "rma", "rover", "rsync", "rtg30", "rufus", "sapo", "sbider", "scooter", "scoutabout", "script", "searchpreview", "searchterms", "seekbot", "serious", "shai", "shelob", "shim-crawler", "sicklebot", "sitecheck", "sitesnagger", "slurpy verifier", "slysearch", "smartdownload", "sna-", "snagger", "snoopy", "sogou", "sootle", "so-net” bat_bot", "spankbot” bat_bot", "spanner” bat_bot", "speeddownload", "spegla", "sphere", "sphider", "spiderbot", "sproose", "sq webscanner", "sqworm", "stamina", "stanford", "studybot", "superbot", "superhttp", "surfbot", "surfwalker", "suzuran", "szukacz", "takeout", "talwinhttpclient", "tarspider", "teleport", "telesoft", "templeton", "testbed", "the intraformant", "thenomad", "tighttwatbot", "titan", "tocrawl/urldispatcher", "true_robot", "turingos", "turnitinbot", "twisted pagegetter", "ucmore", "udmsearch", "umbc", "universalfeedparser", "url control", "urlgetfile", "urly warning", "url_spider_pro", "utilmind", "vayala", "vobsub", "vci", "voideye", "voilabot", "voyager", "w3mir", "web image collector", "web sucker", "web2wap", "webaltbot", "webauto", "webbandit", "webcapture", "webcollage", "webcopier", "webcopy", "webemailextrac", "webenhancer", "webfetch", "webfilter", "webfountain", "webgo", "webleacher", "webminer", "webmirror", "webreaper", "websauger", "websnake", "website", "webstripper", "webvac", "webwalk", "webwhacker", "webzip", "wells search", "wep search 00", "werelatebot", "wget", "whostalking", "widow", "wildsoft surfer", "winhttprequest", "winhttrack", "wumpus", "wwwoffle", "wwwster", "www-collector", "xaldon", "xenu's", "xenus", "xget", "y!tunnelpro", "yahooysmcm", "yadirectbot", "yeti", "zade", "zbot", "zerxbot", "zeus", "zyborg"]

      def accepts?(env)
        !DISALLOWED_USER_AGENTS.include?(env['HTTP_USER_AGENT'].try(:downcase))
      end
    end
  end
end
